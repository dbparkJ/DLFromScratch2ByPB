{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap06 - 게이트가 추가된 RNN\n",
    "\n",
    "5장에서 살펴본 RNN 구조는 순환경로를 통해 과거의 정보를 기억할 수 있었다. 하지만, 실제로는 성능이 좋지 못한데 그 이유는 장기(long term)의존 관계를 잘 학습할 수 없기 때문이다.  이번 장에서는 이러한 RNN의 단점을 보완한 LSTM, GRU에 대해서 다룬다. LSTM이나 GRU에는 **'게이트'**(gate)라는 구조가 더해져 있는데, 이 게이트 덕분에 시계열 데이터의 장기 의존 관계를 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 RNN의 문제점\n",
    "\n",
    "- RNN의 문제점은 시계열 데이터의 장기 의존 관계(long-term dependency)를 학습하기 어렵다.\n",
    "\n",
    "- 그 원인은 BPTT에서 기울기 소실 또는 기울기 폭발(vanishing & exploding gradient)이 일어나기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 RNN 복습\n",
    "\n",
    "<img src=\"./images/fig_6-1.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 계층은 시계열 데이터인 $\\mathbf{x}_{t}$를 입력하면 $\\mathbf{h}_{t}$를 출력한다. \n",
    "\n",
    "- $\\mathbf{h}_{t}$는 RNN 계층의 **은닉 상태(hidden state)**라고 하며, 과거 정보를 저장하는 역할을 한다.\n",
    "\n",
    "- RNN cell의 내부를 자세히 살펴보면 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "<img src=\"./images/fig_6-2.png\" height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 기울기 소실 또는 기울기 폭발\n",
    "\n",
    "<img src=\"./images/fig_6-3.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 예시에서 `\"?\"`에 들어가는 단어는 \"Tom\"이다. \n",
    "\n",
    "- RNNLM이 이 문제에 올바르게 답하려면, \"Tom was watching TV in his room.\"과  \"Mary came into the room.\"이라는 정보를 기억해둬야 한다.\n",
    "\n",
    "- 즉, 이러한 정보를 RNN 레이어의 hidden state에 인코딩해 보관해둬야 한다.\n",
    "\n",
    "- 위의 예시에 대한 RNNLM의 BPTT는 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-4.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 RNN 레이어가 과거 방향으로 기울기(gradient)를 전달함으로써 시간 방향의 의존관계를 학습할 수 있게 된다.\n",
    "\n",
    "- 이때의 기울기는 (이론적으로) 학습해야 할 정보가 들어 있고, 그것을 과거로 전달함으로써 장기 의존 관계를 학습한다.\n",
    "\n",
    "- 하지만, 단순한 RNN(vanilla RNN) 레이어에서는 시간을 거슬러 올라갈수록 기울기가 작아지거나 커지는 문제가 발생한다(vanishing or exploding gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 기울기 소실과 기울기 폭발의 원인\n",
    "\n",
    "<img src=\"./images/fig_6-5.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림은 길이가 $T$인 시계열 데이터를 가정하여 $T$번째 정답 레이블로부터 전해지는 기울기가 어떻게 변화하는지 나타낸 그림이다.\n",
    "\n",
    "- 이때, 시간 방향 기울기(gradient)를 살펴보면, 역전파로 전해지는 기울기는 차례로 `'tanh'` → `'+'` → `'MatMul'` 연산을 통과한다. \n",
    "\n",
    "- `'+'`의 역전파는 상류에서 전해지는 기울기를 그대로 하류로 흘려보내기 때문에 기울기의 값이 변하지 않지만, `'tanh'`와 `'MatMul'`은 변하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh의 역전파\n",
    "\n",
    "$$\n",
    "y = \\tanh{(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\tanh{(x)}}{\\partial x} &= \\frac{( e^x + e^{-x})(e^x + e^{-x}) - ( e^x - e^{-x})( e^x - e^{-x})}{( e^x + e^{-x})^{2}} \\\\ \n",
    "&= 1 - \\frac{( e^x - e^{-x})( e^x - e^{-x})}{( e^x + e^{-x})^{2}} \\\\ \n",
    "&= 1 - \\left\\{ \\frac{( e^x - e^{-x})}{( e^x + e^{-x})} \\right\\}^{2} \\\\\n",
    "&= 1 - \\tanh{(x)}^{2} \\\\\n",
    "&= 1-y^{2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "<img src=\"./images/fig_6-6.png\" height=\"40%\" width=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그래프에서 점선이 $y=\\tanh{(x)}$의 미분이며, 그 값이 1.0 이하이고, $x$가 0으로 부터 멀어질수록 작아진다. \n",
    "\n",
    "- 따라서, 역전파에서 기울기가 $\\tanh$ 노드를 지날때 마다 값은 계속해서 작아지게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MatMul에서의 역전파\n",
    "\n",
    "<img src=\"./images/fig_6-7.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상류로부터 흘러온 기울기 $\\mathbf{dh}$ `'MatMul'`노드에서의 역전파는 $\\mathbf{dh} \\cdot \\mathbf{W_h}^{\\mathsf{T}}$라는 행렬 곱이 된다.\n",
    "\n",
    "- 이러한 행렬곱이 시계열 데이터의 time_step 만큼 반복한다. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:48.374032Z",
     "start_time": "2024-08-01T07:19:48.319077Z"
    }
   },
   "source": [
    "# chap06/rnn_gradient_graph.py\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "plt.rc('font', family=font_name)\n",
    "\n",
    "\n",
    "N = 2  # 미니배치 크기\n",
    "H = 3  # hidden state 벡터의 차원 수\n",
    "T = 20  # 시계열 데이터의 길이(= timestep)\n",
    "\n",
    "dh = np.ones((N, H))\n",
    "np.random.seed(3)\n",
    "# Wh = np.random.randn(H, H) \n",
    "Wh = np.random.randn(H, H) * 0.5 \n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.matmul(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "    \n",
    "u, s, vh = np.linalg.svd(Wh)\n",
    "print(s)\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32971447 0.30503349 0.08888152]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 기울기 폭발 대책\n",
    "\n",
    "- 기울기 폭발(exploding gradient) 해결책으로는 **기울기 클리핑(gradients clipping)**이라는 기법이 있다.\n",
    "\n",
    "- 기울기 클리핑 알고리즘을 의사코드로 나타내면 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\text{if } \\left\\| \\hat{g} \\right\\| \\ge threshold:\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{g} = \\frac{threshold}{\\left\\| \\hat{g} \\right\\|} \\hat{g}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식에서 $\\hat{g}$는 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것이다.\n",
    "    - 예를 들어, $\\mathbf{W}_{1}$과 $\\mathbf{W}_{2}$ 매개변수에 대한 기울기 $\\mathbf{dW}_{1}$과 $\\mathbf{dW}_{2}$를 결합(제곱의 합)한 것이다.\n",
    "    \n",
    "\n",
    "- 위의 식에서 $\\left\\| \\hat{g} \\right\\|$(기울기 L2 노름)가 $threshold$ 값을 초과하면 두 번째 수식과 같이 기울기를 수정한다. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:48.559087Z",
     "start_time": "2024-08-01T07:19:48.545651Z"
    }
   },
   "source": [
    "# chap06/clip_grads.py\n",
    "import numpy as np\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0  # threshold\n",
    "\n",
    "def clip_grads(grds, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:48.700978Z",
     "start_time": "2024-08-01T07:19:48.695979Z"
    }
   },
   "source": [
    "print('before:', dW1.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('after:', dW1.flatten())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [6.49144048 2.78487283 6.76254902 5.90862817 0.23981882 5.58854088\n",
      " 2.59252447 4.15101197 2.83525082]\n",
      "after: [1.49503731 0.64138134 1.55747605 1.36081038 0.05523244 1.28709139\n",
      " 0.59708178 0.95601551 0.65298384]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 기울기 소실과 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 LSTM의 인터페이스\n",
    "\n",
    "- 아래의 그림과 같이 $\\tanh{\\left( \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}} + \\mathbf{x}_{t} \\mathbf{W}_{\\mathbf{x}} + \\mathbf{b} \\right)}$ 를 `tanh`노드로 나타내어 간소화한 것이다.\n",
    "\n",
    "<img src=\"./images/fig_6-10.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN Cell과 LSTM Cell을 간단하게 비교하면 아래의 그림처럼 나타낼 수 있다.\n",
    "\n",
    "<img src=\"./images/fig_6-11.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 보듯 LSTM 계층에는 $\\mathbf{c}$라는 경로가 있다는 것이 RNN과의 차이다. \n",
    "\n",
    "- $\\mathbf{c}$를 **기억 셀**(memory cell)이라고 하며, LSTM의 기억 메커니즘이다.\n",
    "\n",
    "- memory cell의 특징은 LSTM 계층 내에서만 주고받는다는 것이다.\n",
    "    - 다른 계층으로는 출력하지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 LSTM 계층 조립하기\n",
    "\n",
    "- $\\mathbf{c}_{t}$에는 timestep $t$에서의 LSTM의 기억이 저장되어 있는데, 과거로 부터 $t$까지에 필요한 모든 정보가 저장되어 있다고 가정한다.\n",
    "\n",
    "- 이러한 기억 $\\mathbf{c}_{t}$를 바탕으로 특정 연산을 거친 후 외부 계층과 다음 timestep $t+1$의 LSTM에 hidden state $\\mathbf{h}_{t}$를 출력한다. \n",
    "     - 이때 출력하는 $\\mathbf{h}_{t}$는 아래의 그림처럼 memory cell $\\mathbf{c}_{t}$의 값을 $\\tanh$ 함수로 변환한 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-12.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- memory cell $\\mathbf{c}_{t}$와 hidden state $\\mathbf{h}_{t}$의 관계는 $\\mathbf{c}_{t}$의 각 원소에 $\\tanh$ 함수를 적용한 것이다. \n",
    "\n",
    "- 즉, $\\mathbf{c}_{t}$와 $\\mathbf{h}_{t}$의 원소수는 같다는 뜻이고, 예를 들어, $\\mathbf{c}_{t}$의 원소 수가 100개면 $\\mathbf{h}_{t}$의 원소 수도 100개가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 게이트(gate)의 개념\n",
    "\n",
    "- 게이트는 **데이터의 흐름**을 **제어(control)**하는 역할을 한다.\n",
    "\n",
    "- 아래의 그림처럼 게이트의 열림 상태는 `0.0 ~ 1.0` 사이의 실수로 나타나며, '게이트를 얼마나 열까'라는 것도 데이터로 부터 (자동으로) 학습 한다.\n",
    "\n",
    "- 게이트의 열림 상태를 구할 때는 `sigmoid` 함수를 사용하는데, `sigmoid`함수의 출력이 0.0 ~ 1.0의 실수이기 떄문이다.\n",
    "\n",
    "<img src=\"./images/fig_6-14.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM의 게이트들\n",
    "\n",
    "<img src=\"./images/lstm.PNG\" height=\"80%\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 output 게이트\n",
    "\n",
    "\n",
    "- output 게이트는 hidde state($\\mathbf{h}_{t}$)의 출력을 담당한다.\n",
    "\n",
    "- output 게이트는 $\\tanh{(\\mathbf{c}_{t})}$의 각 원소에 대해 '그것이 다음 timestep의 hidde state($\\mathbf{h}_{t}$)에 얼마나 중요한가'를 제어한다.\n",
    "\n",
    "- output 게이트의 열림 상태는 입력 $\\mathbf{x}_{t}$와 이전 상태 $\\mathbf{h}_{t-1}$로 부터 구한다. 아래의 식에서 $\\sigma {()}$는 시그모이드 함수를 의미한다.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = \\sigma \\left( \\mathbf{x}_{t}\\mathbf{W}_{\\mathbf{x}}^{(o)} + \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}}^{(o)} + \\mathbf{b}^{(o)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-15.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 output 게이트의 열림 상태를 계산한 $\\mathbf{o}$는 $\\tanh{\\mathbf{c}_{t}}$의 원소별 곱(element-wise, point-wise, 또는 아다마르 곱)을 통해 $\\mathbf{h}_{t}$를 구하게 된다.\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_{t} = \\mathbf{o} \\odot \\tanh{(\\mathbf{c}_{t})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\tanh$의 출력은 `-1.0 ~ 1.0`의 실수이다. 이 `-1.0 ~ 1.0`의 수치를 그 안에 인코딩된 **'정보'**의 강약(정도)를 표시한다고 해석할 수 있다.\n",
    "- 시그모이드 함수의 출력은 `0.0 ~ 1.0`의 실수이며, 데이터를 얼만큼 통과시킬지를 정하는 비율을 뜻한다.\n",
    "- 따라서, 게이트에서는 시그모이드 함수가 사용되고, 실질적인 '정보'를 지니는 데이터에는 $\\tanh$함수가 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 forget 게이트\n",
    "\n",
    "- forget 게이트는 memory cell에 '무엇을 잊을지'를 제어하는 역할을 한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{f} = \\sigma \\left( \\mathbf{x}_{t} \\mathbf{W}_{x}^{(\\mathbf{f})} + \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}}^{(\\mathbf{f})} + \\mathbf{b}^{(\\mathbf{f})} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- 위의 식에서 구한 $\\mathbf{f}$와 이전 memory cell인 $\\mathbf{c}_{t-1}$과의 원소별 곱, $\\mathbf{c}_{t} = \\mathbf{f} \\odot \\mathbf{c}_{t-1}$을 계산하여 $\\mathbf{c}_{t}$를 구한다.\n",
    "\n",
    "- forget 게이트를 LSTM 레이어에 추가하면 아래의 그림과 같다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-16.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 새로운 기억 셀\n",
    "\n",
    "- 6.2.4에서 forget 게이트를 통해 $t-1$ 시점의 memory cell로 부터 잊어야할 기억을 제어했다.\n",
    "\n",
    "- 이번에는 memory cell에 새로 기억해야할 정보를 아래의 그림과 같이 `tanh` 노드를 추가해 준다.\n",
    "\n",
    "- `tanh`노드가 계산한 결과를 이전 시각의 memory cell $\\mathbf{c}_{t-1}$에 더해짐으로써 새로운 '정보'가 추가된다.\n",
    "\n",
    "- 여기서 `tanh` 노드는 '게이트'가 아니며 단지 새로운 '정보'를 memory cell에 추가하는 것이 목적이다. \n",
    "\n",
    "$$\n",
    "\\mathbf{g} = \\tanh{\\left( \\mathbf{x}_{t} \\mathbf{W}_{x}^{(\\mathbf{g})} + \\mathbf{h}_{t-1} \\mathbf{W}_{h}^{(\\mathbf{g})} + \\mathbf{b}^{(\\mathbf{g})} \\right)}\n",
    "$$\n",
    "\n",
    "- 위에서 계산한 $\\mathbf{g}$(엄밀하게 말하면 $\\mathbf{g}$를 제어하는 input 게이트 $\\mathbf{i}$와의 원소 곱의 결과)가 이전 $t-1$ 시점의 $\\mathbf{c}_{t-1}$에 더해짐으로써 새로운 기억이 생겨난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-17.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6 input 게이트\n",
    "\n",
    "- input 게이트는 $\\mathbf{g}$의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지를 판단하는 역할을 한다.\n",
    "\n",
    "- 즉, 새로운 정보($\\mathbf{g}$)를 바로 더해주는 것이 아니라, input 게이트에 의해 가중된 정보가 새로 추가되는 것이다.\n",
    "\n",
    "$$\n",
    "\\mathbf{i} = \\sigma{\\left( \\mathbf{x}_{t} \\mathbf{W}_{x}^{(\\mathbf{i})} + \\mathbf{h}_{t-1} \\mathbf{W}_{h}^{(\\mathbf{i})} + \\mathbf{b}^{(\\mathbf{i})} \\right)}\n",
    "$$\n",
    "\n",
    "- 위에서 계산한 $\\mathbf{i}$와 $\\mathbf{g}$의 원소별 곱 결과를 memory cell에 더해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-18.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7 LSTM의 기울기 흐름\n",
    "\n",
    "<img src=\"./images/fig_6-19.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림처럼, memory cell의 역전파에서는 '$+$'와 '$\\times$' 노드만을 지나게 된다.\n",
    "\n",
    "- '$+$' 노드는 상류에서 전해지는 기울기를 그대로 흘러보내기 때문에 기울기 변화(감소)는 일어나지 않는다.\n",
    "\n",
    "- '$\\times$' 노드는 '행렬 곱'이 아닌 '원소별 곱(아마다르 곱)'을 계산한다. \n",
    "\n",
    "- '행렬 곱'이 아닌 '원소별 곱'이 이뤄지고, 매 timestep 마다 다른 게이트 값을 이용해 원소별 곱을 계산한다.\n",
    "    - 매번 새로운 게이트 값을 이용하기 때문에 곱셈의 효과가 누적되지 않아 기울기 소실이 일어나지 않게 되는 것이다.\n",
    "\n",
    "\n",
    "- '$\\times$' 노드의 계산은 forget 게이트가 제어한다. \n",
    "    - forget 게이트가 '잊어야 한다'(값이 0에 가까운 경우)고 판단한 memory cell의 원소에 대해서는 기울기가 작아진다.\n",
    "    - forget 게이트가 '잊어서는 안된다' (값이 1에 가까운 경우)고 판단한 원소에 대해서는 기울기가 약화되지 않은채로 과거 방향으로 전해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LSTM은 Long Short-Term Memory의 약자이며, **단기 기억**(short-term memory)을 **긴**(lon)시간 지속할 수 있음을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 LSTM 구현\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{f} &= \\sigma \\left( \\mathbf{x}_{t} \\mathbf{W}_{x}^{(\\mathbf{f})} + \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}}^{(\\mathbf{f})} + \\mathbf{b}^{(\\mathbf{f})} \\right) \\\\\n",
    "\\mathbf{g} &= \\tanh{\\left( \\mathbf{x}_{t} \\mathbf{W}_{x}^{(\\mathbf{g})} + \\mathbf{h}_{t-1} \\mathbf{W}_{h}^{(\\mathbf{g})} + \\mathbf{b}^{(\\mathbf{g})} \\right)}\\\\\n",
    "\\mathbf{i} &= \\sigma{\\left( \\mathbf{x}_{t} \\mathbf{W}_{x}^{(\\mathbf{i})} + \\mathbf{h}_{t-1} \\mathbf{W}_{h}^{(\\mathbf{i})} + \\mathbf{b}^{(\\mathbf{i})} \\right)} \\\\\n",
    "\\mathbf{o} &= \\sigma \\left( \\mathbf{x}_{t}\\mathbf{W}_{\\mathbf{x}}^{(o)} + \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}}^{(o)} + \\mathbf{b}^{(o)} \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{c}_{t} = \\mathbf{f} \\odot \\mathbf{c}_{t-1} + \\mathbf{g} \\odot \\mathbf{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{h}_{t} = \\mathbf{o} \\odot \\tanh{(\\mathbf{c}_{t})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식에서 $\\mathbf{f}, \\mathbf{g}, \\mathbf{i}, \\mathbf{o}$는 활성화 함수안에 아핀(affine)변환을 가지고 있다. 이 식을 아래의 그림처럼 하나의 식으로 정리해서 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-20.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-22.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM 계산 그래프는 아래의 그림과 같다.\n",
    "\n",
    "<img src=\"./images/fig_6-21.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 `slice` 노드의 순전파와 역전파는 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-23.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:49.971398Z",
     "start_time": "2024-08-01T07:19:49.957396Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from common.layers import *\n",
    "from common.functions import sigmoid\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）  \n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "        \n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "        \n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "        \n",
    "        c_next = f * c_prev + g * i  # Ct\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        \n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "        \n",
    "        dc_prev = ds * f\n",
    "        \n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "        \n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "        \n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "        \n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "        \n",
    "        return dx, dh_prev, dc_prev"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Time LSTM 구현\n",
    "\n",
    "<img src=\"./images/fig_6-24.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-25.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:51.303046Z",
     "start_time": "2024-08-01T07:19:51.288038Z"
    }
   },
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 LSTM을 사용한 언어 모델"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:51.396551Z",
     "start_time": "2024-08-01T07:19:51.385551Z"
    }
   },
   "source": [
    "# chap06/rnnlm.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:51.491529Z",
     "start_time": "2024-08-01T07:19:51.464004Z"
    }
   },
   "source": [
    "# chap06/train_rnnlm.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "from rnnlm import Rnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35     # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 기울기 클리핑을 적용하여 학습\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n",
    "\n",
    "# 매개변수 저장\n",
    "model.save_params()"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rnnlm'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m eval_perplexity\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ptb\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrnnlm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Rnnlm\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 하이퍼파라미터 설정\u001B[39;00m\n\u001B[0;32m     12\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'rnnlm'"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 RNNLM 추가 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 LSTM 계층 다층화\n",
    "\n",
    "- 첫 번째 LSTM 레이어의 hidden state $\\mathbf{h}_{t}$가 두 번째 LSTM 레이어의 입력으로 들어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-29.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 드롭아웃에 의한 과적합 억제\n",
    "\n",
    "- 일반적으로 RNN이 단순한 NN보다 쉽게 과적합(overfitting)을 일으킨다.\n",
    "\n",
    "- 과적합을 억제하는 전통적인 방법은 다음과 같다.\n",
    "    - 훈련 데이터의 양 늘리기\n",
    "    - 모델의 복잡도 줄이기\n",
    "    \n",
    "    \n",
    "- RNN계열에서의 드롭아웃(dropout)은 아래의 그림과 같이 **변형 드롭아웃**(Variational Dropout)을 사용한다.\n",
    "\n",
    "- RNN에서의 드롭아웃은 같은 계층에 속한 드롭아웃들은 같은 마스크(mask)를 공유한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-34.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 가중치 공유(weight tying)\n",
    "\n",
    "<img src=\"./images/fig_6-35.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:19:51.774295Z",
     "start_time": "2024-08-01T07:19:51.744291Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ==============================================\n",
    "config.GPU = True\n",
    "# ==============================================\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity, to_gpu\n",
    "from dataset import ptb\n",
    "from better_rnnlm import BetterRnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 650\n",
    "hidden_size = 650\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 3\n",
    "max_grad = 0.25\n",
    "dropout = 0.5\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_val, _, _ = ptb.load_data('val')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "\n",
    "if config.GPU:\n",
    "    corpus = to_gpu(corpus)\n",
    "    corpus_val = to_gpu(corpus_val)\n",
    "    corpus_test = to_gpu(corpus_test)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "best_ppl = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
    "                time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, corpus_val)\n",
    "    print('검증 퍼플렉서티: ', ppl)\n",
    "\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "\n",
    "    model.reset_state()\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'better_rnnlm'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m eval_perplexity, to_gpu\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ptb\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbetter_rnnlm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BetterRnnlm\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# 하이퍼파라미터 설정\u001B[39;00m\n\u001B[0;32m     17\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'better_rnnlm'"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
