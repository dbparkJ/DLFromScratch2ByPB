{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 신경망의 추론",
   "id": "233aa93a2e20ade0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2.1 신경망 추론 전체 그림",
   "id": "8d89bac76eba66ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 완전 연결계층(Fully Connected Layer)\n",
    "신경망(Neural Network)의 중요한 구성 요소 중 하나로, 모든 입력 노드가 모든 출력 노드와 연결되어 있는 계층을 의미한다.  \n",
    "이를 Dense Layer라고도 부른다.  \n",
    "완전 연결계층은 주로 다층 퍼셉트론(Multi-Layer Perceptron, MLP)과 같은 신경망 구조에서 사용된다.  \n",
    "#### 주요 특징\n",
    "1. 완전 연결 : 각 입력 노드는 모든 출력 노드와 연결되어 있다. 따라서 각 출력 노드는 입력 노드의 가중 합을 통해 계산된다.  \n",
    "2. 가중치($W$)와 바이어스($b$) : 각 연결은 고유한 가중치를 가지며, 각 출력 노드는 고유한 바이어스를 가진다.  \n",
    "3. 활성화 함수 : 출력 노드의 값은 활성화 함수를 통해 결정된다.  \n",
    "4. 출력 : 출력 값은 입력 노드의 가중 합과 바이어스를 더한 후 활성화 함수를 적용한 결과이다.  \n",
    "\n",
    "### 행렬의 형상 확인\n",
    "하나의 샘플 데이터 대상이 아닌 학습에서 다수의 샘플 데이터를 처리하기 위해서는 입력 데이터가 일치해야 한다.  \n",
    "그 이유로는 형상에 대응하는 차원 원소 수가 일치해야하는데 이를 활용하기 위해서는 데이터의 일반화가 필요하다(?)"
   ],
   "id": "952997b78f4277ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T05:23:08.233677Z",
     "start_time": "2024-06-23T05:23:08.217593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "완전연결계층에 의한 변환의 미니 배치버전 // 선형 함수\n",
    "'''\n",
    "import numpy as np\n",
    "W1 = np.random.randn(2,4) # 가중치\n",
    "b1 = np.random.randn(4) # 편향\n",
    "x = np.random.randn(10, 2)\n",
    "h = np.matmul(x, W1) + b1\n",
    "print(h)"
   ],
   "id": "aee3323fb96d772c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.18200168e+00  3.28390151e-01  3.90250938e-02 -1.28186664e+00]\n",
      " [-2.13529284e+00 -4.76495172e+00  1.24745964e+00 -1.36748408e+00]\n",
      " [-1.57523245e+00 -1.96648112e+00  3.98731785e-02 -1.87316283e+00]\n",
      " [-7.59694053e-01  3.05661929e+00  7.14873274e-01  1.09265536e-01]\n",
      " [-1.18690488e+00  5.38975334e-01  6.52994451e-01 -6.03289966e-01]\n",
      " [-2.54593708e-01  5.64215760e+00 -2.15873685e-01 -1.69887589e-01]\n",
      " [-8.62625773e-01  2.46311748e+00  7.33579630e-01 -2.48598234e-02]\n",
      " [-1.51273298e+00 -1.27725404e+00  8.72596912e-01 -8.48666389e-01]\n",
      " [-1.27646938e+00  1.63162281e-03  6.15583579e-01 -7.79980346e-01]\n",
      " [-1.47871411e+00 -1.05036188e+00  9.45315844e-01 -7.16184957e-01]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "# 시그모이드 함수를 활용하면 비선형 데이터를 나타낼 수 있고, 이는 활성화 함수 중 하나를 나타낸다.\n",
    "def sifmoid(x) : \n",
    "    return 1/ (1 + np.exp(-x))"
   ],
   "id": "d86e967436beeb03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2.2 계층으로 클래스화 및 순전파 구현\n",
    "책에서는 완전연결계층을 Affine 계층으로, 시그모이드 함수에 의한 변환은 Sigmoid 계층으로 구현 하였음  \n",
    "신경망 추론과정에서 하는 처리를 신경망의 순전파(forward propagation)에 해당된다.  \n",
    "순전파란 말 그대로 입력층에서 출력층으로 향하는 전파를 나타낸다. 반대되는 개념으로는 역전파(back propagation)이다.\n",
    "\n",
    "#### 책의 계층 구현시 '구현 규칙'  \n",
    "모든 계층은 forward() 와 backward() 메서드를 가진다.  \n",
    "모든 계층은 인스턴스 변수인 params와 grads를 가진다.  \n",
    "params는 가중치와 편향과 같은 매개변수를 담는 리스트(list)를 나타낸다.  \n",
    "grads는 params에 저장된 각 매개변수에 대응하여, 해당 매개변수의 기울기를 보관하는 리스트를 나타낸다.  "
   ],
   "id": "4d4ea3ca3d170f8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T05:46:36.533974Z",
     "start_time": "2024-06-23T05:46:36.527975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 시그모이드(Sigmoid) 레이어 구현\n",
    "class Sigmoid:\n",
    "    '''Sigmoid Layer class\n",
    "    \n",
    "    Sigmoid layer에는 학습하는 params가 따로 없으므로 \n",
    "    인스턴스 변수인 params는 빈 리스트로 초기화\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"순전파(forward propagation) 메서드\n",
    "        Args:\n",
    "            x(ndarray): 입력으로 들어오는 값\n",
    "        Returns:\n",
    "            Sigmoid 활성화 값\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "# 완전연결계층(Affine) 구현\n",
    "class Affine:\n",
    "    '''FC layer'''\n",
    "    def __init__(self, W, b):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            W(ndarray): 가중치(weight)\n",
    "            b(ndarray): 편향(bias)\n",
    "        \"\"\"\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"순전파(forward propagation) 메서드\n",
    "        Args:\n",
    "            x(ndarray): 입력으로 들어오는 값\n",
    "        Returns:\n",
    "            out(ndarray): Wx + b\n",
    "        \"\"\"\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        # input -> hidden\n",
    "        W1 = np.random.randn(I, H)  \n",
    "        b1 = np.random.randn(H)\n",
    "         # hidden -> output\n",
    "        W2 = np.random.randn(H, O) \n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        # 레이어 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.parmas = [layer.params for layer in self.layers]\n",
    "        # self.params = []\n",
    "        # for layer in self.layers:\n",
    "        #     self.params += layer.params\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ],
   "id": "bb6d584f68da50ce",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T05:52:30.082042Z",
     "start_time": "2024-06-23T05:52:30.076035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = np.random.randn(10, 2) # 10개의 샘플이 2개의 특성을 가지고 있다라고 생각하면 된다\n",
    "model = TwoLayerNet(2, 4, 3) # 2개의 특성을 가지고, 4개의 은닉층과 3개의 클래스를 분류한다.\n",
    "s = model.predict(x)\n",
    "\n",
    "print(f'input:\\n{x}')\n",
    "print(f'predict :\\n{s}')"
   ],
   "id": "b73726c9cd07d62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[[-1.3416326   1.20690452]\n",
      " [-1.23226156 -0.42801423]\n",
      " [-1.58535409 -0.89558028]\n",
      " [ 0.59670275 -0.7675958 ]\n",
      " [ 0.84956351  1.26291406]\n",
      " [ 1.49416648 -1.17919111]\n",
      " [-0.07371392 -0.20913332]\n",
      " [-2.35206093  0.67837226]\n",
      " [ 0.07313945  1.55841736]\n",
      " [-0.97124501 -1.7813746 ]]\n",
      "predict :\n",
      "[[ 0.15547755  0.96468655 -0.41992304]\n",
      " [ 0.02010249  1.56222132  0.17393831]\n",
      " [-0.0204302   1.61344758  0.28029711]\n",
      " [-0.08696468  1.91235805  0.6590352 ]\n",
      " [ 0.30764588  1.31991877 -0.23993685]\n",
      " [-0.21731824  2.11599916  1.00698894]\n",
      " [ 0.04114777  1.68736734  0.27600336]\n",
      " [ 0.02766646  1.02219403 -0.32087504]\n",
      " [ 0.31253265  1.04542305 -0.43910746]\n",
      " [-0.16131569  1.84865069  0.7648478 ]]\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
